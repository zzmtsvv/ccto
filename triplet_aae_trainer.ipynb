{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Union, Tuple, List, Optional\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class tox_config:\n",
    "    batch_size: int = 64\n",
    "    lr: float = 3e-4\n",
    "    beta1: float = 0.9  # 0.0 for Adam-like optims\n",
    "    beta2: float = 0.999  # 0.9 for Adam-like optims\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    val_size: float = 0.1\n",
    "\n",
    "    # base params to create generator and discriminator\n",
    "    in_channels: int = 1\n",
    "    base_channels: int = 64\n",
    "    latent_dim: int = 128\n",
    "    num_blocks = [2, 2, 2, 2]  # [1, 1, 1, 1]\n",
    "    num_epochs: int = 50\n",
    "    random_seed: int = 42\n",
    "\n",
    "    checkpoint_interval: int = 10\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    img_dir: str = \"generated\"\n",
    "    dataset_path: str = \"triplet_extended_dataset.pt\"  # dataset.pt\n",
    "\n",
    "    generator_size: int = batch_size  # number of generated samples for evaluation (by eyes)\n",
    "    num_dis: int = 1  # number of discriminator updates per iteration\n",
    "    generator_batches: int = 2\n",
    "    num_fakes = batch_size * generator_batches # number of fake examples for single generator update\n",
    "    generator_loss: str = \"mse_loss\"  # ['mse_loss', 'cauchy_loss', 'gemanmcclure_loss', 'welsch_loss', 'l1_loss', 'binary_cross_entropy_loss', 'huber_loss', 'smooth_l1', 'another_smooth_l1_loss']\n",
    "    generator_loss_weights = [0.4, 0.3, 0.3]\n",
    "    use_gram_matrix: bool = False  # either use gram matrix for perception loss or usual L1Loss\n",
    "\n",
    "    vgg16_feature_loss_p_norm: int = 1  # [1, 2]\n",
    "\n",
    "    intensity_mode: str = \"mean\"  # [mean, max]\n",
    "    intensity_reduction: str = \"mean\"  # [mean, sum]\n",
    "    intensity_loss_coef: float = 0.001\n",
    "\n",
    "    num_workers: int = 0\n",
    "\n",
    "    description: str = '''aae with triplet loss upon arcface embeddings including general loss upon\n",
    "                            anchors, positives and negatives (both generator & discriminator)'''\n",
    "    adversarial_term: float = 0.001\n",
    "    triplet_loss_term: float = 0.1\n",
    "\n",
    "    condition_dim: int = 2\n",
    "\n",
    "    model_path: str = \"ArcFaceLoss_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_roll(img, label):\n",
    "    # possible problem: will it work properly if inside the dataloader?\n",
    "    shifts = tuple(np.random.randint(low=0, high=img.shape, size=(2,)))\n",
    "    return img.roll(shifts=shifts, dims=(0,1)), label\n",
    "\n",
    "_Number = Union[float, int]\n",
    "\n",
    "class MaterialPercentDataset(Dataset):\n",
    "    image_area = 64 * 64\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: torch.Tensor,\n",
    "                 labels: torch.Tensor,\n",
    "                 transform=[random_roll,]) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def calc_percent(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        material_count = image.sum()\n",
    "        return material_count / self.image_area\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.images[index]\n",
    "        y = self.labels[index]\n",
    "        if callable(self.transform):\n",
    "            x, y = self.transform(x, y)\n",
    "        elif isinstance(self.transform, list): # erzatz for transfoms.Compose\n",
    "            for tr in self.transform:\n",
    "                x, y = tr(x, y)\n",
    "        \n",
    "        percent = self.calc_percent(x)\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        return x, percent.unsqueeze(-1), y\n",
    "\n",
    "\n",
    "class BinarizedDataset(MaterialPercentDataset):\n",
    "    def __init__(self,\n",
    "                 images: torch.Tensor,\n",
    "                 labels: torch.Tensor,\n",
    "                 bins_number: int = 15,\n",
    "                 transform=[random_roll,]) -> None:\n",
    "        super().__init__(images, labels, transform)\n",
    "\n",
    "        self.bins_number = bins_number\n",
    "\n",
    "        self.linspace = torch.linspace(0, 1, steps=bins_number)\n",
    "    \n",
    "    def one_hot(self, scalar: _Number) -> torch.Tensor:\n",
    "        _, idx = torch.min(torch.abs(self.linspace - scalar), dim=0)\n",
    "        out = torch.zeros(self.bins_number)\n",
    "        out[idx] = 1\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
    "        x = self.images[index]\n",
    "        y = self.labels[index]\n",
    "        if callable(self.transform):\n",
    "            x, y = self.transform(x, y)\n",
    "        elif isinstance(self.transform, list): # erzatz for transfoms.Compose\n",
    "            for tr in self.transform:\n",
    "                x, y = tr(x, y)\n",
    "        \n",
    "        is_broken = y[-1].unsqueeze(0)\n",
    "        percent = self.calc_percent(x).item()\n",
    "        condition = self.one_hot(percent)\n",
    "        condition = torch.cat((condition, is_broken), dim=0)\n",
    "        \n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        return x, condition, y\n",
    "\n",
    "\n",
    "class TripletBinsDataset(BinarizedDataset):\n",
    "    '''\n",
    "        bins_indexes Tensor is supposed to be ids from the extended_clean_dataset.pt\n",
    "    '''\n",
    "    image_area = 64 * 64\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: torch.Tensor,\n",
    "                 labels: torch.Tensor,\n",
    "                 bins_indexes: torch.Tensor,\n",
    "                 transform=[random_roll,]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.images = images\n",
    "        self._labels = labels\n",
    "        self.transform = transform\n",
    "        self.bins_indexes = bins_indexes\n",
    "\n",
    "        self.make_appropriate_labels()\n",
    "    \n",
    "    def make_appropriate_labels(self):\n",
    "        new_labels = []\n",
    "\n",
    "        for label in self._labels:\n",
    "            if label[0].item() == 1.0:\n",
    "                new_labels.append(0)\n",
    "            else:\n",
    "                new_labels.append(1)\n",
    "        \n",
    "        self.labels = torch.tensor(new_labels, dtype=torch.int8)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def make_condition(self,\n",
    "                       x: torch.Tensor) -> torch.Tensor:\n",
    "        percent = self.calc_percent(x).item()\n",
    "        condition = self.one_hot(percent)\n",
    "        return condition\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        anchor = self.images[index]\n",
    "        anchor_label = self.labels[index]\n",
    "        \n",
    "        positive_list = self.bins_indexes[self.bins_indexes != index]\n",
    "        positive_list = positive_list[self.labels[self.bins_indexes != index] == anchor_label]\n",
    "\n",
    "        '''\n",
    "            TODO: ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÑƒ Ð¸Ð· Ð½ÐµÐ³Ð°Ñ‚Ð¸Ð²Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ ÑÐ¾ÑÑ‚Ð¾ÑÑ‚ÑŒ Ð¸Ð· Ñ‚Ð¾Ð³Ð¾ Ð¶Ðµ ÑÐºÐµÐ»ÐµÑ‚Ð°,\n",
    "            Ñ‚Ð¾Ð³Ð´Ð° Ð¾Ð½Ð¸ Ð±ÑƒÐ´ÑƒÑ‚ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒÑÑ Ð² Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð±Ð¸Ð½Ð°Ñ…. Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ð¾Ð¿Ñ†Ð¸Ð¸ Ð½ÐµÑ‚, Ñ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ \n",
    "            Ð±ÐµÑ€ÐµÐ¼ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÑƒ Ð¸Ð· Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð±Ð¸Ð½Ð¾Ð².\n",
    "        '''\n",
    "        negative_list = self.bins_indexes[self.bins_indexes == index]\n",
    "        negative_list = negative_list[self.labels[self.bins_indexes == index] != anchor_label]\n",
    "        \n",
    "        if not len(negative_list):\n",
    "            negative_list = self.bins_indexes[self.bins_indexes != index]\n",
    "            negative_list = negative_list[self.labels[self.bins_indexes == index] != anchor_label]\n",
    "        \n",
    "        positive = self.images[random.choice(positive_list)]\n",
    "        negative = self.images[random.choice(negative_list)]\n",
    "\n",
    "        anchor_condition = self.make_condition(anchor)\n",
    "        pos_condition = self.make_condition(positive)\n",
    "        neg_condition = self.make_condition(negative)\n",
    "\n",
    "        for tr in self.transform:\n",
    "            anchor, _  = tr(anchor, None)\n",
    "            positive, _ = tr(positive, None)\n",
    "            negative, _ = tr(negative, None)\n",
    "        \n",
    "        if anchor.ndim == 2:\n",
    "            anchor = anchor.unsqueeze(0)\n",
    "            positive = positive.unsqueeze(0)\n",
    "            negative = negative.unsqueeze(0)\n",
    "        \n",
    "        return (anchor, anchor_condition) (positive, pos_condition) (negative, neg_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DTYPE = torch.float32\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = tox_config.random_seed) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def make_dir(dir_path):\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps=1e-12):\n",
    "    return x / (x.pow(2).sum() + eps).sqrt()\n",
    "\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.reshape(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAEDiscriminatorLoss(_Loss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                target_scores: torch.Tensor,\n",
    "                encoder_scores: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        labels_true = torch.ones_like(target_scores)\n",
    "        labels_fake = torch.zeros_like(encoder_scores)\n",
    "\n",
    "        loss = self.loss(target_scores, labels_true) + self.loss(encoder_scores, labels_fake)\n",
    "        return loss / 2\n",
    "\n",
    "\n",
    "class AAEGeneratorLoss(_Loss):\n",
    "    width_dim = 64\n",
    "    height_dim = 64\n",
    "\n",
    "    def __init__(self,\n",
    "                 adversarial_term: float,\n",
    "                 weights: List[float],\n",
    "                 size_average=None,\n",
    "                 reduce=None,\n",
    "                 reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "        self.adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "        self.pixelwise_loss = nn.L1Loss()\n",
    "        self.tiling_loss = nn.L1Loss()\n",
    "\n",
    "        self.alpha = adversarial_term\n",
    "\n",
    "        self.general_coef, self.vertical_coef, self.horizontal_coef = weights\n",
    "\n",
    "    def forward(self,\n",
    "                encoder_scores: torch.Tensor,\n",
    "                decoded_images: torch.Tensor,\n",
    "                real_images: torch.Tensor) -> torch.Tensor:\n",
    "        labels_true = torch.ones_like(encoder_scores)\n",
    "\n",
    "        adversarial_loss = self.adversarial_loss(encoder_scores, labels_true)\n",
    "        reconstruction_term = self.pixelwise_loss(decoded_images, real_images)\n",
    "\n",
    "        base_loss = self.alpha * adversarial_loss + (1 - self.alpha) * reconstruction_term\n",
    "        vertical_loss = self.vertical_loss(decoded_images)\n",
    "        horizontal_loss = self.horizontal_loss(decoded_images)\n",
    "\n",
    "        return self.general_coef * base_loss + vertical_loss + horizontal_loss\n",
    "\n",
    "    def vertical_loss(self, tile: torch.Tensor) -> torch.Tensor:\n",
    "        upper_line = tile[:, :, 0, :]\n",
    "        lower_line = tile[:, :, self.height_dim - 1, :]\n",
    "\n",
    "        return self.vertical_coef * self.tiling_loss(upper_line, lower_line)\n",
    "\n",
    "    def horizontal_loss(self, tile: torch.Tensor) -> torch.Tensor:\n",
    "        left_line = tile[:, :, :, 0]\n",
    "        right_line = tile[:, :, :, self.width_dim - 1]\n",
    "\n",
    "        return self.horizontal_coef * self.tiling_loss(left_line, right_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape: Tuple[int, int, int, int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.reshape(self.shape)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.block(x)\n",
    "        out += self.skip_connection(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 downsample: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_activation = nn.ReLU()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1 + downsample, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        if in_channels == out_channels and not downsample:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            skip = [nn.AvgPool2d(2)] if downsample else []\n",
    "            skip.append(nn.Conv2d(in_channels, out_channels, 1, 1, 0))\n",
    "            self.skip_connection = nn.Sequential(*skip)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.block(self.first_activation(x))\n",
    "        out += self.skip_connection(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "        Architecture is the same as in Discriminator except the head projects to the latent space with latent_dim\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 base_channels: int = 64,\n",
    "                 latent_dim: int = 128,\n",
    "                 num_blocks: List[int] = [1, 1, 1, 1]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        first_dis_block = DiscriminatorBlock(in_channels, base_channels, downsample=True)\n",
    "        first_dis_block.first_activation = nn.Identity()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            first_dis_block,\n",
    "            self.make_layer(base_channels, 2 * base_channels, num_blocks[0]),\n",
    "            self.make_layer(2 * base_channels, 4 * base_channels, num_blocks[1]),\n",
    "            self.make_layer(4 * base_channels, 8 * base_channels, num_blocks[2]),\n",
    "            self.make_layer(8 * base_channels, 16 * base_channels, num_blocks[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(16 * base_channels, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, a=0.0, mode='fan_in', nonlinearity=\"leaky_relu\"\n",
    "                )\n",
    "\n",
    "    @staticmethod\n",
    "    def make_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   num_blocks: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            DiscriminatorBlock(in_channels, out_channels, downsample=True),\n",
    "            *[DiscriminatorBlock(out_channels, out_channels) for _ in range(num_blocks - 1)]\n",
    "        )\n",
    "\n",
    "    def get_model_size(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 condition_dim: int,\n",
    "                 in_channels: int,\n",
    "                 base_channels: int = 64,\n",
    "                 latent_dim: int = 128,\n",
    "                 num_blocks: List[int] = [1, 1, 1, 1]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 256 * base_channels, bias=False),\n",
    "            Reshape((-1, 16 * base_channels, 4, 4)),\n",
    "            self.make_layer(16 * base_channels, 8 * base_channels, num_blocks[0]),\n",
    "            self.make_layer(8 * base_channels, 4 * base_channels, num_blocks[1]),\n",
    "            self.make_layer(4 * base_channels, 2 * base_channels, num_blocks[2]),\n",
    "            self.make_layer(2 * base_channels, base_channels, num_blocks[3]),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, in_channels, 1, 1, 0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, a=0.0, mode='fan_in', nonlinearity=\"leaky_relu\"\n",
    "                )\n",
    "\n",
    "    @staticmethod\n",
    "    def make_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   num_blocks: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            DecoderBlock(in_channels, out_channels),\n",
    "            *[DecoderBlock(out_channels, out_channels) for _ in range(num_blocks - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "    def sample(self, num_samples: int, noise: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        if noise is None:\n",
    "            noise = torch.randn((num_samples, self.latent_dim))\n",
    "\n",
    "        return self.forward(noise.to(device))\n",
    "\n",
    "    def get_model_size(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class AAEDiscriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 condition_dim: int,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int = 64) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)\n",
    "\n",
    "    def enable_grads(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def disable_grads(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self,\n",
    "                 start_dim: int = 0,\n",
    "                 end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.flatten(x, self.start_dim, self.end_dim)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        https://arxiv.org/abs/1603.05027\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels, num_channels // 4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_channels // 4, num_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.net(x)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 hidden_channels: int = 256) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_net = nn.Sequential(\n",
    "            self.make_block(in_channels, hidden_channels // 16),\n",
    "            self.make_block(hidden_channels // 16, hidden_channels // 8),\n",
    "            self.make_block(hidden_channels // 8, hidden_channels // 4),\n",
    "            self.make_block(hidden_channels // 4, hidden_channels // 2),\n",
    "            self.make_block(hidden_channels // 2, hidden_channels),\n",
    "            Flatten(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 3)\n",
    "        )\n",
    "        # self.head = nn.Linear(3, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_block(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 4,\n",
    "                   stride: int = 2,\n",
    "                   padding: int = 1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            ResidualBlock(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        embeddings = self.projection_net(x)\n",
    "        # scores = self.head(embeddings)\n",
    "\n",
    "        return None, embeddings\n",
    "\n",
    "    def get_model_size(self) -> int:\n",
    "        return sum([p.numel() for p in self.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletEmbeddingLoss(_Loss):\n",
    "    '''\n",
    "        Triplet loss realized over embeddings of some representative network\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 feature_extractor: nn.Module,\n",
    "                 margin: float = 1.0,\n",
    "                 swap: bool = True,\n",
    "                 size_average=None,\n",
    "                 reduce=None,\n",
    "                 reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.disable_feature_extractor_grads()\n",
    "\n",
    "        self.margin = margin\n",
    "        self.swap = swap\n",
    "    \n",
    "    def disable_feature_extractor_grads(self):\n",
    "        for p in self.feature_extractor.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def calc_euclidean(self,\n",
    "                       x1: torch.Tensor,\n",
    "                       x2: torch.Tensor) -> torch.Tensor:\n",
    "        _, x1 = self.feature_extractor(x1)\n",
    "        _, x2 = self.feature_extractor(x2)\n",
    "\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    \n",
    "    def casual_loss(self,\n",
    "                    anchor: torch.Tensor,\n",
    "                    positive: torch.Tensor,\n",
    "                    negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "\n",
    "        # this ensures that the hardest negative inside the triplet is used for backpropagation\n",
    "        if self.swap:\n",
    "            distance_negative_a = self.calc_euclidean(anchor, negative)\n",
    "            distance_negative_p = self.calc_euclidean(positive, negative)\n",
    "            \n",
    "            distance_negative = torch.minimum(distance_negative_a, distance_negative_p)\n",
    "        else:\n",
    "            distance_negative = self.calc_euclidean(anchor, negative)\n",
    "\n",
    "\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses\n",
    "    \n",
    "    def forward(self,\n",
    "                anchor: torch.Tensor,\n",
    "                positive: torch.Tensor,\n",
    "                negative: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        loss = self.casual_loss(anchor, positive, negative)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "\n",
    "\n",
    "class TripletImageLoss(_Loss):\n",
    "    '''\n",
    "        Triplet loss upon generations grom the generator.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 feature_extractor=None,\n",
    "                 margin: float = 1.0,\n",
    "                 swap: bool = True,\n",
    "                 use_gram_matrix: bool = False,\n",
    "                 size_average=None,\n",
    "                 reduce=None,\n",
    "                 reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "        self.margin = margin\n",
    "        self.use_gram_matrix = use_gram_matrix\n",
    "        self.swap = swap\n",
    "        \n",
    "        self.loss_fn = self.calc_euclidean\n",
    "        if use_gram_matrix:\n",
    "            self.loss_fn = self.gram_matrix_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_euclidean(x1: torch.Tensor,\n",
    "                       x2: torch.Tensor,\n",
    "                       dims: Tuple[int] = (1, 2, 3)) -> torch.Tensor:\n",
    "        # dims = list(range(1, len(x1.shape)))\n",
    "        return (x1 - x2).pow(2).sum(dim=dims).unsqueeze(1)\n",
    "    \n",
    "    def casual_loss(self,\n",
    "                    anchor: torch.Tensor,\n",
    "                    positive: torch.Tensor,\n",
    "                    negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.loss_fn(anchor, positive)\n",
    "\n",
    "        # this ensures that the hardest negative inside the triplet is used for backpropagation\n",
    "        if self.swap:\n",
    "            distance_negative_a = self.loss_fn(anchor, negative)\n",
    "            distance_negative_p = self.loss_fn(positive, negative)\n",
    "            \n",
    "            distance_negative = torch.minimum(distance_negative_a, distance_negative_p)\n",
    "        else:\n",
    "            distance_negative = self.loss_fn(anchor, negative)\n",
    "\n",
    "\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_gram_matrix(y: torch.Tensor) -> torch.Tensor:\n",
    "        b, Ñ, h, w = y.shape\n",
    "        return torch.einsum('bchw,bdhw->bcd', [y, y]) / (h * w)\n",
    "    \n",
    "    def gram_matrix_loss(self,\n",
    "                         x: torch.Tensor,\n",
    "                         y: torch.Tensor) -> torch.Tensor:\n",
    "        assert len(x.shape) == 4\n",
    "        # batch_size, c, h, w = x.shape\n",
    "\n",
    "        G = self.compute_gram_matrix(x)\n",
    "        A = self.compute_gram_matrix(y)\n",
    "        #return A.shape\n",
    "\n",
    "        return (G - A).pow(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletAAETrainer:\n",
    "    def __init__(self,\n",
    "                 cfg=tox_config()) -> None:\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "        seed_everything(cfg.random_seed)\n",
    "#         self.checkpoint_path = os.path.join(cfg.checkpoint_dir, f\"conditional_aae.pt\")\n",
    "\n",
    "        make_dir(cfg.checkpoint_dir)\n",
    "        make_dir(cfg.img_dir)\n",
    "\n",
    "        self.encoder = Encoder(cfg.in_channels, cfg.base_channels, cfg.latent_dim, cfg.num_blocks).to(self.device)\n",
    "        self.generator = Decoder(cfg.condition_dim, cfg.in_channels, cfg.base_channels, cfg.latent_dim, cfg.num_blocks).to(self.device)\n",
    "        self.discriminator = AAEDiscriminator(cfg.condition_dim, cfg.latent_dim).to(self.device)\n",
    "\n",
    "        self.gen_optim = torch.optim.AdamW(chain(self.encoder.parameters(), self.generator.parameters()),\n",
    "                                           lr=cfg.lr,\n",
    "                                           betas=(cfg.beta1, cfg.beta2))\n",
    "        self.discr_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=cfg.lr, betas=(cfg.beta1, cfg.beta2))\n",
    "\n",
    "        #self.gen_criterion = GeneratorLoss()\n",
    "        self.gen_criterion = AAEGeneratorLoss(cfg.adversarial_term, cfg.generator_loss_weights)\n",
    "        self.discr_criterion = AAEDiscriminatorLoss()\n",
    "\n",
    "        # init dataset and dataloader\n",
    "        imgs, labels, indexes = torch.load(cfg.dataset_path)\n",
    "        traindata = TripletBinsDataset(imgs, labels, indexes)\n",
    "        self.train_loader = DataLoader(\n",
    "            traindata, shuffle=True, batch_size=cfg.batch_size, drop_last=True, pin_memory=False, num_workers=cfg.num_workers\n",
    "        )\n",
    "\n",
    "        feature_extractor = Classifier().to(self.device)\n",
    "        arcface_state_dict = torch.load(cfg.model_path)\n",
    "        feature_extractor.load_state_dict(arcface_state_dict)\n",
    "\n",
    "        self.triplet_criterion = TripletEmbeddingLoss(feature_extractor)\n",
    "\n",
    "    def fit(self):\n",
    "        run_name = f\"conditional_tiling_bce_\" + str(self.cfg.random_seed)\n",
    "        print(f\"Training starts on {self.cfg.device} ðŸš€\")\n",
    "\n",
    "        with wandb.init(project=\"topology_topxpy\", group=f\"triplet_embedding\", name=run_name, job_type=\"training\"):\n",
    "            wandb.config.update({k: v for k, v in self.cfg.__dict__.items() if not k.startswith('__')})\n",
    "\n",
    "            gen_noise = torch.randn((self.cfg.generator_size, self.generator.latent_dim)).to(self.device)\n",
    "\n",
    "            fixed_condition = torch.zeros(self.cfg.generator_size, self.cfg.condition_dim)\n",
    "            fixed_condition[:32] = torch.tensor([1.0, 0.0])\n",
    "            fixed_condition[32:] = torch.tensor([0.0, 1.0])\n",
    "            fixed_condition = fixed_condition.to(self.device)\n",
    "\n",
    "\n",
    "            for e in range(self.cfg.num_epochs):\n",
    "                self.encoder.train()\n",
    "                self.generator.train()\n",
    "                self.discriminator.train()\n",
    "\n",
    "                total_discriminator_loss = 0\n",
    "                total_generator_loss = 0\n",
    "                total_triplet_loss = 0\n",
    "                total_overall_generator_loss = 0\n",
    "                total_discriminator_counter = 0\n",
    "                total_generator_counter = 0\n",
    "\n",
    "                with tqdm(self.train_loader, desc=f\"{e + 1}/{self.cfg.num_epochs} epochs\", total=self.cfg.num_epochs) as t:\n",
    "                    #self.generator.train()\n",
    "\n",
    "                    for i, (anchor_tuple, pos_tuple, neg_tuple) in enumerate(self.train_loader):\n",
    "                        self.encoder.train()\n",
    "                        self.generator.train()\n",
    "                        self.discriminator.train()\n",
    "\n",
    "                        anchor, anchor_condition = anchor_tuple\n",
    "                        positive, positive_condition = pos_tuple\n",
    "                        negative, negative_condition = neg_tuple\n",
    "\n",
    "                        anchor, anchor_condition = anchor.to(self.device), anchor_condition.to(self.device)\n",
    "                        positive, positive_condition = positive.to(self.device), positive_condition.to(self.device)\n",
    "                        negative, negative_condition = negative.to(self.device), negative_condition.to(self.device)\n",
    "\n",
    "                        size = anchor.shape[0]\n",
    "\n",
    "                        # generator step\n",
    "                        self.gen_optim.zero_grad()\n",
    "                        self.discriminator.disable_grads()\n",
    "\n",
    "                        encoded_anchor = self.encoder(anchor)\n",
    "                        decoded_anchor = self.generator(encoded_anchor, anchor_condition)\n",
    "\n",
    "                        encoded_positive = self.encoder(positive)\n",
    "                        decoded_positive = self.generator(encoded_positive, anchor_condition)\n",
    "\n",
    "                        encoded_negative = self.encoder(negative)\n",
    "                        decoded_negative = self.generator(encoded_negative, anchor_condition)\n",
    "\n",
    "                        encoder_scores_anchor = self.discriminator(encoded_anchor, anchor_condition)\n",
    "                        encoder_scores_positive = self.discriminator(encoded_positive, positive_condition)\n",
    "                        encoder_scores_negative = self.discriminator(encoded_negative, negative_condition)\n",
    "\n",
    "\n",
    "                        generator_loss_anchor = self.gen_criterion(encoder_scores_anchor, decoded_anchor, anchor)\n",
    "                        generator_loss_positive = self.gen_criterion(encoder_scores_positive, decoded_positive, positive)\n",
    "                        generator_loss_negative = self.gen_criterion(encoder_scores_negative, decoded_negative, negative)\n",
    "                        generator_loss = (generator_loss_anchor + generator_loss_positive + generator_loss_negative) / 3\n",
    "\n",
    "                        triplet_loss = self.triplet_criterion(decoded_anchor, decoded_positive, decoded_negative)\n",
    "                        overall_generator_loss = (1 - self.cfg.triplet_loss_term) * generator_loss + self.cfg.triplet_loss_term * triplet_loss\n",
    "\n",
    "                        overall_generator_loss.backward()\n",
    "                        self.gen_optim.step()\n",
    "\n",
    "                        total_generator_loss += generator_loss.item() * size\n",
    "                        total_generator_counter += size\n",
    "                        total_triplet_loss += triplet_loss.item() * size\n",
    "                        total_overall_generator_loss += overall_generator_loss.item() * size\n",
    "\n",
    "                        self.discriminator.enable_grads()\n",
    "\n",
    "                        # discriminator step\n",
    "                        self.discr_optim.zero_grad()\n",
    "\n",
    "                        z = torch.randn(size, self.cfg.latent_dim).to(self.device)\n",
    "\n",
    "                        target_scores_anchor = self.discriminator(z, anchor_condition)\n",
    "                        target_scores_positive = self.discriminator(z, positive_condition)\n",
    "                        target_scores_negative = self.discriminator(z, negative_condition)\n",
    "\n",
    "                        encoder_scores_anchor = self.discriminator(encoded_anchor.detach(), anchor_condition)\n",
    "                        encoder_scores_positive = self.discriminator(encoded_positive.detach(), positive_condition)\n",
    "                        encoder_scores_negative = self.discriminator(encoded_negative.detach(), negative_condition)\n",
    "\n",
    "                        discriminator_loss_anchor = self.discr_criterion(target_scores_anchor, encoder_scores_anchor)\n",
    "                        discriminator_loss_positive = self.discr_criterion(target_scores_positive, encoder_scores_positive)\n",
    "                        discriminator_loss_negative = self.discr_criterion(target_scores_negative, encoder_scores_negative)\n",
    "\n",
    "                        discriminator_loss = (discriminator_loss_anchor + discriminator_loss_positive + discriminator_loss_negative) / 3\n",
    "\n",
    "                        discriminator_loss.backward()\n",
    "                        self.discr_optim.step()\n",
    "\n",
    "                        total_discriminator_loss += discriminator_loss.item() * size\n",
    "                        total_discriminator_counter += size\n",
    "\n",
    "                        t.set_postfix({\n",
    "                            \"discr_loss\": total_discriminator_loss / total_discriminator_counter,\n",
    "                            \"gen_loss\": total_generator_loss / total_generator_counter,\n",
    "                        })\n",
    "\n",
    "                        if not i % self.cfg.checkpoint_interval:\n",
    "                            wandb.log({\n",
    "                                \"discriminator_loss\": total_discriminator_loss / total_discriminator_counter,\n",
    "                                \"generator_loss\": total_generator_loss / total_generator_counter,\n",
    "                                \"overall_generator_loss\": total_overall_generator_loss / total_generator_counter,\n",
    "                                \"triplet_loss\": total_triplet_loss / total_generator_counter\n",
    "                            })\n",
    "\n",
    "                        if i == len(self.train_loader) - 1:\n",
    "                            self.generator.eval()\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                generated_images = self.generator.sample(self.cfg.generator_size, gen_noise, fixed_condition).cpu()\n",
    "                            generated_images = make_grid(\n",
    "                                generated_images, nrow=8, normalize=True, value_range=(-1, 1)).numpy().transpose(1, 2, 0)\n",
    "                            plt.imsave(os.path.join(self.cfg.img_dir, f\"{e + 1}.jpg\"), generated_images)\n",
    "\n",
    "                            if not (e + 1) % self.cfg.checkpoint_interval:\n",
    "                                pass\n",
    "                                # self.save()\n",
    "\n",
    "    def save(self):\n",
    "        state_dict = {\n",
    "            \"generator\": self.generator.state_dict(),\n",
    "            \"discriminator\": self.discriminator.state_dict(),\n",
    "            \"generator_optim\": self.gen_optim.state_dict(),\n",
    "            \"discriminator_optim\": self.discr_optim.state_dict(),\n",
    "            \"encoder\": self.encoder.state_dict()\n",
    "            }\n",
    "        torch.save(state_dict, self.checkpoint_path)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=self.device)\n",
    "\n",
    "        self.generator.load_state_dict(state_dict[\"generator\"])\n",
    "        self.discriminator.load_state_dict(state_dict[\"discriminator\"])\n",
    "        self.gen_optim.load_state_dict(state_dict[\"generator_optim\"])\n",
    "        self.discr_optim.load_state_dict(state_dict[\"discriminator_optim\"])\n",
    "        self.encoder.load_state_dict(state_dict[\"encoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
